{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPVzsvfJgQkvhatsC7jgtPd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nh0875/Desafios-Procesamiento-del-Habla-974/blob/main/Desafio_Datasets_C4_Hilliard.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zq6j8LsYq1Dr"
      },
      "source": [
        "## **Vectorización de texto y modelo de clasificación Naïve Bayes con el dataset 20 newsgroups**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "l7cXR6CI30ry"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import numpy as np\n",
        "newsgroups_train=fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
        "newsgroups_test=fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'))\n",
        "tfidfvect=TfidfVectorizer(max_df=0.8, min_df=2, ngram_range=(1, 2), sublinear_tf=True)\n",
        "newsgroups_train.data[0]\n",
        "X_train=tfidfvect.fit_transform(newsgroups_train.data)\n",
        "X_test=tfidfvect.transform(newsgroups_test.data)\n",
        "print(type(X_train))\n",
        "print(f'shape: {X_train.shape}')\n",
        "print(f'cantidad de documentos: {X_train.shape[0]}')\n",
        "print(f'tamaño del vocabulario (dimensionalidad de los vectores): {X_train.shape[1]}')\n",
        "idx2word={v: k for k,v in tfidfvect.vocabulary_.items()}\n",
        "y_train=newsgroups_train.target\n",
        "y_train[:10]\n",
        "print(f'clases {np.unique(newsgroups_test.target)}')\n",
        "newsgroups_test.target_names"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1) Vectorizar Documentos**"
      ],
      "metadata": {
        "id": "nQ4mzD5Z-MCG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Qlc-tLc5NaS",
        "outputId": "13663442-88cb-4962-c676-7dadd179e35d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document ID: 5\n",
            "Most similar documents:\n",
            "Document ID: 3, Similarity: 0.0578\n",
            "Document ID: 14, Similarity: 0.0535\n",
            "Document ID: 0, Similarity: 0.0462\n",
            "Document ID: 1, Similarity: 0.0436\n",
            "Document ID: 7, Similarity: 0.0391\n",
            "\n",
            "Document ID: 14\n",
            "Most similar documents:\n",
            "Document ID: 5, Similarity: 0.0535\n",
            "Document ID: 3, Similarity: 0.0512\n",
            "Document ID: 12, Similarity: 0.0498\n",
            "Document ID: 18, Similarity: 0.0448\n",
            "Document ID: 7, Similarity: 0.0351\n",
            "\n",
            "Document ID: 11\n",
            "Most similar documents:\n",
            "Document ID: 14, Similarity: 0.0267\n",
            "Document ID: 8, Similarity: 0.0252\n",
            "Document ID: 12, Similarity: 0.0218\n",
            "Document ID: 5, Similarity: 0.0206\n",
            "Document ID: 3, Similarity: 0.0199\n",
            "\n",
            "Document ID: 12\n",
            "Most similar documents:\n",
            "Document ID: 14, Similarity: 0.0498\n",
            "Document ID: 3, Similarity: 0.0415\n",
            "Document ID: 5, Similarity: 0.0350\n",
            "Document ID: 1, Similarity: 0.0316\n",
            "Document ID: 18, Similarity: 0.0265\n",
            "\n",
            "Document ID: 10\n",
            "Most similar documents:\n",
            "Document ID: 7, Similarity: 0.0183\n",
            "Document ID: 5, Similarity: 0.0139\n",
            "Document ID: 9, Similarity: 0.0135\n",
            "Document ID: 12, Similarity: 0.0132\n",
            "Document ID: 3, Similarity: 0.0125\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "if X_test.shape[0] >= 20:\n",
        "    random_indices=random.sample(range(20), 5)\n",
        "    selected_docs=X_test[random_indices]\n",
        "    similarities=cosine_similarity(selected_docs, X_test[:20])\n",
        "\n",
        "    for idx, sim in zip(random_indices, similarities):\n",
        "        similar_indices=np.argsort(sim)[-6:-1][::-1]\n",
        "        print(f\"Document ID: {idx}\")\n",
        "        print(\"Most similar documents:\")\n",
        "        for sim_idx in similar_indices:\n",
        "            print(f\"Document ID: {sim_idx}, Similarity: {sim[sim_idx]:.4f}\")\n",
        "        print()\n",
        "else:\n",
        "    print(\"Not enough documents in X_test.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2) Entrenar el Modelo de Clasificacion**"
      ],
      "metadata": {
        "id": "oE0p4cuxMd3G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "X_train, X_val, y_train, y_val=train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "models={\n",
        "    \"MultinomialNB\": MultinomialNB(),\n",
        "    \"ComplementNB\": ComplementNB()\n",
        "}\n",
        "for model_name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred=model.predict(X_val)\n",
        "    f1=f1_score(y_val, y_pred, average='macro')\n",
        "    print(f\"{model_name} F1 Score: {f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQ8SXlV3EXqL",
        "outputId": "e2646abf-1a18-4035-e381-7d8d189cb4b8"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MultinomialNB F1 Score: 0.5718\n",
            "ComplementNB F1 Score: 0.7314\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3) Transponer Matriz y estudiar la similaridad**"
      ],
      "metadata": {
        "id": "Ds-RtYexMlW_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_term_doc=X_train.T"
      ],
      "metadata": {
        "id": "odQtB6TC_E6_"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "words_to_check=[\"mouse\", \"science\", \"technology\", \"news\", \"sports\"]\n",
        "\n",
        "word_indices=[tfidfvect.vocabulary_.get(word) for word in words_to_check if word in tfidfvect.vocabulary_]\n",
        "\n",
        "for word, idx in zip(words_to_check, word_indices):\n",
        "    if idx is not None:\n",
        "        word_vector=tfidfvect.transform([word])\n",
        "        similarities=cosine_similarity(word_vector, X_train).flatten()\n",
        "        similar_indices=np.argsort(similarities)[-10:][::-1]\n",
        "        print(f\"Word being considered: {word}\")\n",
        "        print(\"Most similar words:\")\n",
        "        count=0\n",
        "        for sim_idx in similar_indices:\n",
        "            similar_word=idx2word[sim_idx]\n",
        "            if re.match(\"^[A-Za-z]+$\", similar_word):\n",
        "                print(f\"Word: {similar_word}, Similarity: {similarities[sim_idx]:.4f}\")\n",
        "                count += 1\n",
        "            if count == 5:\n",
        "                break\n",
        "        print()\n",
        "    else:\n",
        "        print(f\"The word '{word}' is not in the vocabulary.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLC8olhDF3wy",
        "outputId": "83eca018-dfcb-4f05-b211-1024b4a8c021"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word being considered: mouse\n",
            "Most similar words:\n",
            "Word: darwin, Similarity: 0.4084\n",
            "Word: beb, Similarity: 0.3933\n",
            "Word: akron, Similarity: 0.3720\n",
            "Word: axis, Similarity: 0.3354\n",
            "Word: beginning, Similarity: 0.3296\n",
            "\n",
            "Word being considered: science\n",
            "Most similar words:\n",
            "Word: couriers, Similarity: 0.2383\n",
            "Word: busch, Similarity: 0.2255\n",
            "Word: console, Similarity: 0.2118\n",
            "Word: ashland, Similarity: 0.2089\n",
            "Word: airframe, Similarity: 0.2029\n",
            "\n",
            "Word being considered: technology\n",
            "Most similar words:\n",
            "Word: arianespace, Similarity: 0.3920\n",
            "Word: anarchists, Similarity: 0.3652\n",
            "Word: crunching, Similarity: 0.2950\n",
            "Word: bandwagon, Similarity: 0.2934\n",
            "Word: damp, Similarity: 0.2318\n",
            "\n",
            "Word being considered: news\n",
            "Most similar words:\n",
            "Word: archival, Similarity: 0.3344\n",
            "Word: cora, Similarity: 0.3103\n",
            "Word: aloud, Similarity: 0.3103\n",
            "Word: ceilings, Similarity: 0.2972\n",
            "Word: awesley, Similarity: 0.2918\n",
            "\n",
            "Word being considered: sports\n",
            "Most similar words:\n",
            "Word: atf, Similarity: 0.3238\n",
            "Word: cache, Similarity: 0.3093\n",
            "Word: beter, Similarity: 0.3016\n",
            "Word: chaudhary, Similarity: 0.2936\n",
            "Word: amber, Similarity: 0.2718\n",
            "\n"
          ]
        }
      ]
    }
  ]
}